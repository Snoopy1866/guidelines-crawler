name: Crawl

on:
  schedule:
    - cron: "0 */6 * * *"
  workflow_dispatch:
    inputs:
      page:
        description: "Page to crawl"
        required: true
        default: "0"

permissions:
  contents: write
  pull-requests: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Crawler
        run: |
          if [ "${{github.event_name }}" == "schedule" ]; then
            python -m crawler --page 0
          else
            python -m crawler --page ${{ github.event.inputs.page }}
          fi

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Update guidelines"
          branch: "update-guidelines"
          title: "Update guidelines"
          body: "This PR updates the guidelines by crawling from page ${{ github.event.inputs.page }}."
